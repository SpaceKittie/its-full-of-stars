```
 _ 
 \`*-.    
  )  _`-. 
 .  : `. . 
 : _   '  \ 
 ; *` _.   `*-._
 `-.-'          `-.
   ;       `       `.
   :.       .        :
   . \  .   :   .-'   . 
   '  `+.;  ;  '      :
   :  '  |    ;       ;-.
   ; '   : :`-:     _.`* ;
 .*' /  .*' ; .*`- +'  `*'
*-*   `*-*   `*-*'
```
# ğŸš€ Snowflake ELT Dashboard

A batch data pipeline and analytics platform that connects to space-related APIs, processes the data through Snowflake, and creates an interactive dashboard.

![Dashboard Preview](images/dash1.png)
![Dashboard Preview](images/dash2.png)
![Dashboard Preview](images/dash3.png)

## ğŸŒŸ Features

- **Real-time ISS tracking** with speed and location visualization
- **Astronomy Picture of the Day** from NASA's API
- **Astronaut database** showing who's currently in space
- **Fully automated data pipeline** using Airflow with dynamic DAG generation
- **Transformations in Snowflake** using dbt Core

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  External  â”‚    â”‚          â”‚    â”‚           â”‚    â”‚            â”‚
â”‚   APIs     â”‚â”€â”€â”€â–ºâ”‚  Airflow â”‚â”€â”€â”€â–ºâ”‚ Snowflake â”‚â”€â”€â”€â–ºâ”‚ Streamlit  â”‚
â”‚            â”‚    â”‚          â”‚    â”‚           â”‚    â”‚            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
![Dashboard Preview](images/airflow.png)

The project implements a full ELT pipeline:
- **Extract**: API data is fetched using dynamic DAGs and Python scripts orchestrated by Airflow
- **Load**: Raw data is loaded into Snowflake through the same orchestration framework
- **Transform**: Data is modeled using dbt Core with a layered approach

## ğŸš€ Quick Start with GitHub Codespaces

The fastest way to explore this project is with GitHub Codespaces, which automatically sets up the development environment.

[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/spacekittie/its-full-of-stars)

The Makefile will automatically trigger Terraform infrastructure creation on Snowflake and appropriate roles and permissions required for the project as part of the setup process (e.g., when running `make all` or `make infra-apply`).

### Steps in Codespaces:

1. **Set up environment**:
   ```bash
   # Copy the example environment file
   cp .env.example .env
   # Edit .env with your Snowflake credentials
   ```
2. **Run the full setup**:
   ```bash
   make all
   ```
3. **Access Airflow** via the Codespaces ports tab (port 8080)
   - Trigger the DAGs to start data collection

4. **Launch the dashboard**:
   ```bash
   make dashboard
   ```

## ğŸ’» Local Development Setup

### Prerequisites

- Docker (for running Airflow via Astro Runtime)
- Snowflake account with admin privileges
- Python 3.9+ (if running Streamlit locally)

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/spacekittie/its-full-of-stars.git
   cd its-full-of-stars
   ```
2. **Set up environment**:
   ```bash
   # Copy the example environment file
   cp .env.example .env
   # Edit .env with your Snowflake credentials
   ```
3. **Run the full setup**:
   ```bash
   make all
   ```
4. **Access Airflow** via the Codespaces ports tab (port 8080)
   - Trigger the DAGs to start data collection

5. **Launch the dashboard**:
   ```bash
   make dashboard
   ```

## ğŸ§° Project Structure

```
.
â”œâ”€â”€ astro/                  # Airflow project directory
â”‚   â”œâ”€â”€ dags/               # Airflow DAGs
â”‚   â”‚   â””â”€â”€ api_factory.py  # Dynamic DAG generator
â”‚   â”œâ”€â”€ dbt/                # dbt project
â”‚   â”‚   â”œâ”€â”€ models/         # Data models
â”‚   â”‚   â”‚   â”œâ”€â”€ airlock/    # Intermediate models
â”‚   â”‚   â”‚   â””â”€â”€ mission_control/ # Final models
â”‚   â””â”€â”€ include/            # Python modules for DAGs
â”‚       â””â”€â”€ utils/          # Shared utilities
â”‚           â”œâ”€â”€ api_strategy.py  # Strategy pattern implementation
â”‚           â””â”€â”€ snowflake_loader.py # Data loading utility
â”œâ”€â”€ infra/                  # Terraform configuration
â”œâ”€â”€ .env.example            # Example environment variables
â”œâ”€â”€ Makefile                # Project automation
â”œâ”€â”€ streamlit_app.py        # Dashboard application
â””â”€â”€ README.md               # This documentation
```

## ğŸ› ï¸ Make Commands

| Command | Description |
|---------|-------------|
| `make all` | Full setup: infrastructure, dbt init, and Airflow startup |
| `make setup` | Generate Terraform and Streamlit config from .env |
| `make infra-apply` | Apply Terraform changes to Snowflake |
| `make dbt-init` | Initialize raw tables in Snowflake |
| `make up` | Start Airflow  |
| `make down` | Stop Airflow |
| `make dbt-run` | Run dbt transformations |
| `make dashboard` | Start Streamlit dashboard |

## ğŸ“ Notes

- When running locally outside of Codespaces, ensure all prerequisites are properly installed
- The project is configured for automation in Codespaces, so local setup might need adjustments
- Check Airflow logs if data isn't showing up in the dashboard
- Tear down infrastructure when done: `make infra-destroy`
- Clean up the generated files: `make clean`
